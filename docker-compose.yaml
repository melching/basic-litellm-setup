services:

  ### litellm ###
  litellm-postgres:
    image: postgres
    restart: always
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: llmproxy
      POSTGRES_PASSWORD: dbpassword9090
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d litellm -U llmproxy"]
      interval: 1s
      timeout: 5s
      retries: 10
    volumes:
      - litellm_postgres_data:/var/lib/postgresql/data

  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    restart: unless-stopped
    command:
      - "--config=/litellm_config.yaml"
      - "--detailed_debug"
    environment:
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      DATABASE_URL: postgresql://llmproxy:dbpassword9090@litellm-postgres:5432/litellm
    ports:
      - "4000:4000"
    volumes:
      - ./litellm_config.yaml:/litellm_config.yaml


  ### open web ui ###
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    ports:
      - 3123:8080
    volumes:
      - open_webui_data:/app/backend/data
    environment:
      OPENAI_API_KEY: ${LITELLM_MASTER_KEY}
      OPENAI_API_BASE_URL: http://litellm:4000/v1


  ### vllm ###
  # vllm:
  #   image: vllm/vllm-openai:v0.7.1
  #   ports:
  #     - "8082:8000"
  #   entrypoint: python3
  #   command: -m vllm.entrypoints.openai.api_server --enforce-eager --gpu-memory-utilization 0.7 --model Qwen/Qwen2.5-32B-Instruct-AWQ --max-model-len 10000
  #   volumes:
  #     - hf-cache:/root/.cache/huggingface
  #   restart: always
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #         - driver: nvidia
  #           count: all
  #           capabilities: [gpu]

  ### ollama ###
  #...


volumes:
  open_webui_data:
  litellm_postgres_data:
  hf-cache: